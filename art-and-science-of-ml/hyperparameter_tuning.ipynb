{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rotary-alpha",
   "metadata": {},
   "source": [
    "# Performing the Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-boring",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "1. Learn how to use $\\texttt{cloudml-hypertune}$ to report the results for Cloud hyperparameter tuning trial runs\n",
    "2. Learn how to configure the $\\texttt{.yaml}$ file for submitting a Cloud hyperparameter tuning job\n",
    "3. Submit a hyperparameter tuning job to Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-medline",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-restaurant",
   "metadata": {},
   "source": [
    "Let's see if we can improve upon that by tuning our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-given",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that are set *prior* to training a model, as opposed to parameters which are learned *during* training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-verification",
   "metadata": {},
   "source": [
    "These include learning rate and batch size, but also model design parameters such as type of activation function and number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-suspension",
   "metadata": {},
   "source": [
    "Here are the four most common ways to finding the ideal hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-luxury",
   "metadata": {},
   "source": [
    "1. Manually\n",
    "2. Grid Search\n",
    "3. Random Search\n",
    "4. Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-longer",
   "metadata": {},
   "source": [
    "**1. Manual**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-terrorist",
   "metadata": {},
   "source": [
    "Traditionally, hyperparameters tuning is a manual trial and error process. An ML engineer has some intuition about suitable hyperparameters which they use as a starting point, then they observe the result and use that information to try a new set of hyperparameters to try to beat the existing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-discipline",
   "metadata": {},
   "source": [
    "Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-stability",
   "metadata": {},
   "source": [
    "- Educational, builds up your intuition as an ML engineer\n",
    "- Inexpensive because only one trial is conducted at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-tracker",
   "metadata": {},
   "source": [
    "Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-remedy",
   "metadata": {},
   "source": [
    "- Requires a lot of time and patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-muslim",
   "metadata": {},
   "source": [
    "**2. Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-parade",
   "metadata": {},
   "source": [
    "On the other extreme we can use grid search. Define a discrete set of values to try for each hyperparameter then try every possible combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-saver",
   "metadata": {},
   "source": [
    "Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-district",
   "metadata": {},
   "source": [
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Guaranteed to find the best solution within the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-little",
   "metadata": {},
   "source": [
    "Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-melbourne",
   "metadata": {},
   "source": [
    "- Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-audit",
   "metadata": {},
   "source": [
    "**4. Bayesian Optimisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-adoption",
   "metadata": {},
   "source": [
    "Unlike Grid Search and Random Search, Bayesian Optimisation takes into account information from past trials to select parameters for future trials. More information can be found [here](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-tissue",
   "metadata": {},
   "source": [
    "Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-settlement",
   "metadata": {},
   "source": [
    "- Picks values intelligently based on results from past trials\n",
    "- Less expensive because requires fewer trials to get a good result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-milan",
   "metadata": {},
   "source": [
    "Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-equivalent",
   "metadata": {},
   "source": [
    "- Requires sequential trials for best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-domestic",
   "metadata": {},
   "source": [
    "**AI Platform HyperTune**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-baghdad",
   "metadata": {},
   "source": [
    "AI Platform HyperTune, powered by [Google Vizier](https://ai.google/research/pubs/pub46180), uses Bayesian Optimisation by default, but [also supports](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview#search_algorithms) Grid Search and Random Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-annual",
   "metadata": {},
   "source": [
    "When tuning just a few hyperparameters (say less than $4$), Grid Search and Random Search work well, but when tuning several hyperparameters and the search space is large, Bayesian Optimisation is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with your own bucket and project below\n",
    "BUCKET = '<BUCKET>'\n",
    "PROJECT = '<PROJECT>'\n",
    "REGION = '<YOUR REGION>'\n",
    "\n",
    "OUTDIR = 'gs://{bucket}/taxifare/data'.format(bucket=BUCKET)\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['OUTDIR'] = OUTDIR\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '2.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Setting up Cloud SDK properties\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-avenue",
   "metadata": {},
   "source": [
    "## Make code compatible with AI Platform Training Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-stopping",
   "metadata": {},
   "source": [
    "In order to make our code compatible with AI Platform Training Service we need to make the following changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-stereo",
   "metadata": {},
   "source": [
    "1. Upload data to Google Cloud Storage\n",
    "2. Move code into a trainer Python package\n",
    "3. Submit training job with `gcloud` to train on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-prison",
   "metadata": {},
   "source": [
    "## Upload data to Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-future",
   "metadata": {},
   "source": [
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-conviction",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = biquery.Client(project=PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset('taxifare'))\n",
    "\n",
    "# Create a data set\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print('Dataset created')\n",
    "except:\n",
    "    print('Dataset already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-physics",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%biquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    \"unusued\" AS key\n",
    "FROM \n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "AND\n",
    "    fare_amount >= 2.5\n",
    "AND\n",
    "    pickup_longitude > -78\n",
    "AND\n",
    "    pickup_longitude < -70\n",
    "AND\n",
    "    dropoff_longitude > -78\n",
    "AND\n",
    "    dropoff_longitude < -70\n",
    "AND\n",
    "    pickup_latitude > 37\n",
    "AND\n",
    "    pickup_latitude < 45\n",
    "AND\n",
    "    dropoff_latitude > 37\n",
    "AND\n",
    "    dropoff_latitude < 45\n",
    "AND\n",
    "    passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-experiment",
   "metadata": {},
   "source": [
    "Make the validation data set be 1/10 the size of the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    \"unusued\" AS key\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "AND\n",
    "    fare_alout >= 2.5\n",
    "AND\n",
    "    pickup_longitude > -78\n",
    "AND\n",
    "    pickup_longitude < -70\n",
    "AND\n",
    "    dropoff_longitude > -78\n",
    "AND\n",
    "    dropoff_longitude < -70\n",
    "AND\n",
    "    pickup_latitude > 37\n",
    "AND\n",
    "    pickup_latitude < 45\n",
    "AND\n",
    "    dropoff_latitude > 37\n",
    "AND\n",
    "    dropoff_latitude < 45\n",
    "AND\n",
    "    passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-latino",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "    --destination_format CSV \\\n",
    "    --field_delimiter \",\" --noprint_header \\\n",
    "    taxifare.feateng_training_data \\\n",
    "    $OUTDIR\\taxi-train-*.csv\n",
    "    \n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "    --destination_format CSV \\\n",
    "    --field_delimiter \",\" --noprint_header /\n",
    "    taxifare.feateng_valid_data \\\n",
    "    $OUTDIR/taxi-valid-*.csv\n",
    "    \n",
    "# List the files of the bucket\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a short header for each object\n",
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-numbers",
   "metadata": {},
   "source": [
    "If all ran smoothly, you should be able to list the data bucket by running the following commad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the files of the bucket\n",
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-metabolism",
   "metadata": {},
   "source": [
    "## Move code into Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-morning",
   "metadata": {},
   "source": [
    "Here, we moved our code into a Python package for training on Cloud AI Platform. Let's just check that the files are there. You should see the following files in the `taxifare/trainer` directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-least",
   "metadata": {},
   "source": [
    "- `__init__.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-chorus",
   "metadata": {},
   "source": [
    "- `model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-tract",
   "metadata": {},
   "source": [
    "- `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will list all the files in the mentioned directory with a long listing format\n",
    "!ls -la taxifare/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-hollywood",
   "metadata": {},
   "source": [
    "To use hyperparameter tuning in your training job you must perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-profile",
   "metadata": {},
   "source": [
    "1. Specify the hyperparameter tuning configuration for your training job by including a `HyperparameterSpec` in your `TrainingInput` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-lawyer",
   "metadata": {},
   "source": [
    "2. Include the following code in your training application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-passenger",
   "metadata": {},
   "source": [
    "- Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial. Add your hyperparameter metric to the summary for your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-pension",
   "metadata": {},
   "source": [
    "- To submit a hyperparameter tuning job, we must modify `model.py` and `task.py` to expose any variables we want to tune as command line arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-thing",
   "metadata": {},
   "source": [
    "## Modify `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "\n",
    "# Importing the necessary modeules\n",
    "import datetime\n",
    "import hypertune\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "logging.info(tf.__version__)\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\"\n",
    "]\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "DAYS = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "\n",
    "# Split features and labels from feature dictionary\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in [\"key\"]:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "# Load data set using the `tf.data` API from CSV files\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat)\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "# Prefetch overlaps the processing and model execution of a training setp\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size, num_repeat=1):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "# Parse a string and return a `datetime.datetime`\n",
    "def parse_datetime(s):\n",
    "    if type(s) is not str:\n",
    "        s = s.numpy().decode(\"utf-8\")\n",
    "    return datetime.datetime.striptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "# Here, `tf.sqrt` computes element-wise square root of the input tensor\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff**2 + latdiff**2)\n",
    "\n",
    "# `timestamp.weekday()` function returns the day of the week represented by the date in the given `timestamp`\n",
    "def get_dayofweek(s):\n",
    "    ts = parse_datetime(s)\n",
    "    return DAYS[ts.weekday()]\n",
    "\n",
    "# It wraps a Python function into a TensorFlow op that executes it eagerly\n",
    "@tf.function\n",
    "def dayofweeek(ts_in):\n",
    "    return tf.map_fn(\n",
    "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string),\n",
    "        ts_in)\n",
    "\n",
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed[\"pickup_datetime\"]\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    \n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in [\"pickup_longitude\", \"dropoff_longitude\"]:\n",
    "        transformed[lon_col] = tf.keras.layers.Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name=\"scale_{}\".format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "        \n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in [\"pickup_latitude\", \"dropoff_latitude\"]:\n",
    "        transformed[lat_col] = tf.keras.layers.Lambda(\n",
    "            lambda: (x - 37)/8.0,\n",
    "            name=\"scale_{}\".format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "        \n",
    "    # Adding Euclidean distance (no need to be accurate: NN will calibrate it)\n",
    "    transformed[\"euclidean\"] = tf.keras.layers.Lambda(euclidean, name=\"euclidean\")[\n",
    "        inputs[\"pickup_longitude\"],\n",
    "        inputs[\"pickup_latitude\"],\n",
    "        inputs[\"dropoff_longitude\"],\n",
    "        inputs[\"dropoff_latitude\"]\n",
    "    ]\n",
    "    feature_columns[\"euclidean\"] = tf.feature_column.numeric_column(\"euclidean\")\n",
    "    \n",
    "    # Get hour of day from timestamp of form \"2010-02-08 09:17:00+00:00\"\n",
    "    transformed[\"hourofday\"] = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "        tf.strings.substr(x, 11, 2), out_type=df.dtypes.int32),\n",
    "        name=\"hourofday\"\n",
    "    )(inputs[\"pickup_datetime\"])\n",
    "    feature_columns[\"hourofday\"] = tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_identity(\n",
    "            \"hourofday\", num_buckets=24))\n",
    "    \n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"pickup_latitude\"], latbuckets\n",
    "    )\n",
    "    b_dlat = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"dropoff_latitude\"], latbuckets\n",
    "    )\n",
    "    b_plon = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"pickup_longitude\"], lonbuckets\n",
    "    )\n",
    "    b_dlon = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"dropoff_longitude\"], lonbuckets\n",
    "    )\n",
    "    ploc = tf.feature_column.crossed_column(\n",
    "        [b_plat, b_plon], nbuckets * nbuckets\n",
    "    )\n",
    "    dloc = tf.feature_column.crossed_column(\n",
    "        [b_dlat, b_dlon], nbuckets * nbuckets\n",
    "    )\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    feature_columns[\"pickup_and_dropoff\"] = tf.feature_column.embedding_column(\n",
    "        pd_pair, 100\n",
    "    )\n",
    "    \n",
    "    return transformed, feature_columns\n",
    "\n",
    "# Here, `tf.sqrt` computes element-wise square root of the input tensor\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr):\n",
    "    # Input layer is all float except for `pickup_datetime` which is a string\n",
    "    STRING_COLS = [\"pickup_datetime\"]\n",
    "    NUMERIC_COLS = (\n",
    "        set(CSV_COLUMNS) - set([LABEL_COLUMN, \"key\"]) - set(STRING_COLS)\n",
    "    )\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype=\"float32\")\n",
    "        for colname in STRING_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: layer.Input(name=colname, shape=(), dtype=\"string\")\n",
    "    })\n",
    "    \n",
    "    # Transforms\n",
    "    transformed, feature_columns = transform(\n",
    "        inputs, NUMERIC_COLS, STRING_COLS, nbuckets=nbuckets)\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "    \n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = layers.Dense(nodes, activation=\"relu\", name=\"h{}\".format(layer))(x)\n",
    "    output = layers.Dense(1, name=\"fare\")(x)\n",
    "    \n",
    "    model = models.Model(inputs, output)\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define train and evaluate method to evaluate performance of the model\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    nnsize = hparams[\"nnsize\"]\n",
    "    nbuckets = hparams[\"nbuckets\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "    \n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "        \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    savedmodel_dir = os.path.join(output_dir, \"savedmodel\")\n",
    "    model_export_path = os.path.join(savedmodel_dir, timestamp)\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "    \n",
    "    dnn_model = build_dnn_model(nbuckets, nnsize, lr)\n",
    "    logging.info(dnn_model.summary())\n",
    "    \n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "    \n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "    \n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                              save_weigths_only=True,\n",
    "                                              verbose=1)\n",
    "    \n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path,\n",
    "                                           histogram_freq=1)\n",
    "    \n",
    "    history = dnn_model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2, # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb]\n",
    "    )\n",
    "    \n",
    "    # Exporting the model with default serving function\n",
    "    tf.saved_model.save(dnn_model, model_export_path)\n",
    "    \n",
    "    hp_metric = history.history[\"val_rmse\"][num_evals-1]\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag=\"rmse\",\n",
    "        metric_value=hp_metric,\n",
    "        global_set=num_evals\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-illustration",
   "metadata": {},
   "source": [
    "## Modify task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "# Importing the necessary modules\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Batch size for training steps\",\n",
    "        type = int,\n",
    "        default = 32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help = \"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs = \"+\",\n",
    "        type = int,\n",
    "        default = [32, 8]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnbuckets\",\n",
    "        help = \"Number of buckets to divide lat and lon width\",\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        help = \"Learning rate for optimizer\",\n",
    "        type = float,\n",
    "        default = 0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help = \"Number of times to evaluate model on eval data training\",\n",
    "        type = int,\n",
    "        default = 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help = \"Number of examples to train on\",\n",
    "        type = int,\n",
    "        default = 100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS location pattern of train files containing eval URLs\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"This model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    hparams = args.__dict__\n",
    "    hparams[\"output_dir\"] = os.path.join(\n",
    "        hparams[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "    print(\"output_dir\", hparams[\"output_dir\"])\n",
    "    model.train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-montgomery",
   "metadata": {},
   "source": [
    "## Create `config.yaml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-bridge",
   "metadata": {},
   "source": [
    "Specify the hyperparameter tuning configuration for your training job. Create a `HyperparameterSpec` object to hold the hyperparameter tuning configuration for your training job, and add the `HyperparameterSpec` as the hyperparameters object in your `TrainingInput` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-archive",
   "metadata": {},
   "source": [
    "In your `HyperparameterSpec`, set the `hyperparameterMetricTag` to a value representing your chosen metric. If you don't specify a `hyperparameterMetricTag`, AI Platform Training looks for a metric with the name `training/hptuning/metric`. The following example shows how to create a configuration for a metric named `metric1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hptuning_config.yaml\n",
    "\n",
    "# Setting parameters for hptuning_config.yaml\n",
    "trainingInput:\n",
    "    scaleTier: BASIC\n",
    "    hyperparameters:\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 10\n",
    "        maxParallelTrials: 2\n",
    "        hyperparameterMetricTag: rmse\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: lr\n",
    "        type: DOUBLE\n",
    "        minValue: 0.0001\n",
    "        maxValue: 0.1\n",
    "        scaleType: UNIT_LOG_SCALE\n",
    "        - parameterName: nbuckets\n",
    "        type: INTEGER\n",
    "        minValue: 10\n",
    "        maxValue: 25\n",
    "        scaleType: UNIT_LINEAR_SCALE\n",
    "        - parameterName: batch_size\n",
    "        type: DISCRETE\n",
    "        discreteValues:\n",
    "        - 15\n",
    "        - 30\n",
    "        - 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-lease",
   "metadata": {},
   "source": [
    "**Report your hyperparameter metric to AI Platform Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-exception",
   "metadata": {},
   "source": [
    "The way to report your hyperparameter metric to the AI Platform Training service depends on whether you are using TensorFlow for training or not. It also depends on whether you are using a runtime version or a custom container for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-evening",
   "metadata": {},
   "source": [
    "We recommend that your training code reports your hyperparameter metric to AI Platform Training frequently in order to take advantage of early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-infection",
   "metadata": {},
   "source": [
    "*TensorFlow with a runtime version*: If you use an AI Platform Training runtime version and train with TensorFlow, then you can report hour hyperparameter metric to AI Platform Training by writing the metric to a TensorFlow summary. Use one of the following functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-pressure",
   "metadata": {},
   "source": [
    "You may need to install `cloudml-hypertune` on your machine to run this code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the latest version of the package\n",
    "!pip install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Testing your training code locally\n",
    "EVAL_DATA_PATH=./taxifare/tests/data/taxi-valid*\n",
    "TRAIN_DATA_PATH=./taxifare/tests/data/taxi-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "\n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize 32 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls taxifare-model/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$PROJECT_ID\n",
    "REGION=\"us-central1\"\n",
    "TFVERSION=\"2.1\"\n",
    "\n",
    "# Output directory and job ID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u + %y%m%d_%H%M%S)\n",
    "JOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=15\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=10\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS path\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOBID \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=taxifare/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --config=hptuning_config.yaml \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --region=${REGION} \\\n",
    "    -- \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data_path $TRAIN_DATA_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "    --num_evals $NUM_EVALS \\\n",
    "    --nbuckets $NBUCKETS \\\n",
    "    --lr $LR \\\n",
    "    --nnsize $NNSIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

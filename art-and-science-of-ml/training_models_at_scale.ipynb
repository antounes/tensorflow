{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sorted-graham",
   "metadata": {},
   "source": [
    "# Training Models at Scale with AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-computer",
   "metadata": {},
   "source": [
    "**Learning objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-individual",
   "metadata": {},
   "source": [
    "1. Learn how to organise your training code into a Python package\n",
    "2. Train your model using a cloud infrastructure via Google Cloud AI Platform Training\n",
    "3. Learn how to run your training package using Docker containers and push training Docker images to a Docker registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-thickness",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-newark",
   "metadata": {},
   "source": [
    "In this notebook we'll make the jump from training locally to training in the cloud. We'll take advantage of Google Cloud's [AI Platform Training](https://cloud.google.com/ai-platform/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-college",
   "metadata": {},
   "source": [
    "AI Platform Training is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handled seamlessly by the managed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery=1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"<BUCKET>\"\n",
    "PROJECT = \"<PROJECT>\"\n",
    "REGION = \"<YOUR REGION>\"\n",
    "\n",
    "OUTDIR = \"gs://{bucket}/taxifare/data\".format(bucket=BUCKET)\n",
    "\n",
    "# Store the values of `BUCKET`, `OUTDIR`, `PROJECT`, `REGION` and `TFVERSION` in environment variables\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"OUTDIR\"] = OUTDIR\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-sullivan",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BigQuery dataset for our data\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print(\"Dataset created\")\n",
    "except:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-drill",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "# Creating the table in our dataset\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amout + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pikcup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    \"unusued\" AS key\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "AND\n",
    "    fare_amount >= 2.5\n",
    "AND\n",
    "    pickup_longitude > -78\n",
    "AND\n",
    "    pickup_longitude < -70\n",
    "AND\n",
    "    dropoff_longitude > -78\n",
    "AND\n",
    "    dropoff_longitude < -70\n",
    "AND\n",
    "    pickup_latitude > 37\n",
    "AND\n",
    "    pickup_latitude < 45\n",
    "AND\n",
    "    dropoff_latitude > 37\n",
    "AND\n",
    "    dropoff_latitude < 45\n",
    "AND\n",
    "    passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-feeding",
   "metadata": {},
   "source": [
    "Make the validation data set be 1/10 the size of the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    \"unusued\" AS key\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "AND\n",
    "    fare_amount >= 2.5\n",
    "AND\n",
    "    pickup_longitude > -78\n",
    "AND\n",
    "    pickup_longitude < -70\n",
    "AND\n",
    "    dropoff_longitude > -78\n",
    "AND\n",
    "    dropoff_longitude < -70\n",
    "AND\n",
    "    pickup_latitude > 37\n",
    "AND\n",
    "    pickup_latitude < 45\n",
    "AND\n",
    "    dropoff_latitude > 37\n",
    "AND\n",
    "    dropoff_latitude < 45\n",
    "AND\n",
    "    passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-walter",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Deleting the contents of the output directory\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "# Fetching the training data to output directory\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "    --destination_format CSV \\\n",
    "    --field_delimiter \",\" --noprint_header \\\n",
    "    taxifare.feateng_training_data \\\n",
    "    $OUTDIR/taxi-train-*.csv\n",
    "    \n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "    --destination_format CSV \\\n",
    "    --field_delimiter \",\" --noprint_header \\\n",
    "    taxifare.feateng_valid_data \\\n",
    "    $OUTDIR/taxi-valid-*.csv\n",
    "    \n",
    "# The `ls` command will show the content of working directory\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `cat` command will output the contents of one or more URLs\n",
    "# Using `head -2` we are showing only top two output files\n",
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-given",
   "metadata": {},
   "source": [
    "## Make code compatible with AI Platform Training Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-married",
   "metadata": {},
   "source": [
    "In order to make our code compatible with AI Platform Training we need to make the following chages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-softball",
   "metadata": {},
   "source": [
    "1. Upload data to Google Cloud Storage\n",
    "2. Move code into a trainer Python package\n",
    "3. Submit training job with `gcloud` to train on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-implementation",
   "metadata": {},
   "source": [
    "### Upload data to Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-lotus",
   "metadata": {},
   "source": [
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `ls` command will show the content of working directory\n",
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-american",
   "metadata": {},
   "source": [
    "### Move code into a Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-designer",
   "metadata": {},
   "source": [
    "The first thing to do is to convert your training code snippets into a regular Python package that we will then `pip install` into the Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-couple",
   "metadata": {},
   "source": [
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialisation code but for our purposes an empty file suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-curtis",
   "metadata": {},
   "source": [
    "#### Create the package directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-playlist",
   "metadata": {},
   "source": [
    "Our package directory contains $3$ files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hazardous-midnight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘taxifare/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir taxifare/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wicked-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antounes/Documents/tensorflow/art-and-science-of-ml/taxifare\n"
     ]
    }
   ],
   "source": [
    "cd taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beautiful-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tired-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.py\r\n"
     ]
    }
   ],
   "source": [
    "ls taxifare/trainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-arrival",
   "metadata": {},
   "source": [
    "#### Paste existing code into `model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-buyer",
   "metadata": {},
   "source": [
    "A Python package requires our code to be in a `.py` file, as opposed to notebook cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accessible-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/model.py\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "logging.info(tf.__version__)\n",
    "\n",
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\"\n",
    "]\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "DAYS = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in [\"key\"]:\n",
    "        row_data.pop(unwanted_col)\n",
    "    # The `.pop()` method will return item and drom from DataFrame\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    # The `tf.data.experimental.make_csv_dataset()` method reads CSV files into a `tf.data.Dataset`\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat\n",
    "    )\n",
    "    # The `.map()` function executes a specified function for each item in an iterable\n",
    "    # The item is sent to the function as a parameter\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    # The `.prefetch()` method will start a background thread to populate an ordered buffer that acts\n",
    "    # like a queue, so that downstream pipeline stages aren't paused\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "def parse_datetime(s):\n",
    "    if type(s) is not str:\n",
    "        s = s.numpy().decode(\"utf-8\")\n",
    "    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff ** 2 + latdiff ** 2)\n",
    "\n",
    "def get_dayofweek(s):\n",
    "    ts = parse_datetime(s)\n",
    "    return DAYS[ts.weekday()]\n",
    "\n",
    "@tf.function\n",
    "def dayofweek(ts_in):\n",
    "    return tf.map_fn(\n",
    "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string),\n",
    "        ts_in\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def fare_thresh(x):\n",
    "    return 60 * tf.activations.relu(x)\n",
    "\n",
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed[\"pickup_datetime\"]\n",
    "    \n",
    "    feature_columns = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    \n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in [\"pickup_longitude\", \"dropoff_longitude\"]:\n",
    "        transformed[lon_col] = tf.keras.layers.Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name=\"scale_{}\".format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "        \n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in [\"pickup_latitude\", \"dropoff_latitude\"]:\n",
    "        transformed[lat_col] = tf.keras.layers.Lambda(\n",
    "            lambda x: (x - 37)/8.0,\n",
    "            name=\"scale_{}\".format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "        \n",
    "    # Adding Euclidean distance (no need to be accurate: NN will calibrate it)\n",
    "    transformed[\"euclidean\"] = tf.keras.layers.Lambda(euclidean, name=\"euclidean\")([\n",
    "        inputs[\"pickup_longitude\"],\n",
    "        inputs[\"pickup_latitude\"],\n",
    "        inputs[\"dropoff_longitude\"],\n",
    "        inputs[\"dropoff_latitude\"]\n",
    "    ])\n",
    "    feature_columns[\"euclidean\"] = tf.feature_column.numeric_column(\"euclidean\")\n",
    "    \n",
    "    # Hour of day from timestamp of form `2010-02-08 09:17:00+00:00`\n",
    "    transformed[\"hourofday\"] = layers.Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "            tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32),\n",
    "        name=\"hourofday\"\n",
    "    )(inputs[\"pickup_datetime\"])\n",
    "    feature_columns[\"hourofday\"] = tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_identity(\n",
    "            \"hourofday\", num_buckets=24))\n",
    "    \n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"pickup_latitude\"], latbuckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"dropoff_latitude\"], latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"pickup_longitude\"], lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(\n",
    "        feature_columns[\"dropoff_longitude\"], lonbuckets)\n",
    "    ploc = tf.feature_column.crossed_column(\n",
    "        [b_plat, b_plon], nbuckets**2)\n",
    "    dloc = tf.feature_column.crossed_column(\n",
    "        [b_dlat, b_dlon], nbuckets**2)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets**4)\n",
    "    feature_columns[\"pickup_and_dropoff\"] = tf.feature_column.embedding_column(\n",
    "        pd_pair, 100)\n",
    "    \n",
    "    return transformed, feature_columns\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr):\n",
    "    # Input layer is all float except for `pickup_datetime` which is a string\n",
    "    STRING_COLS = [\"pickup_datetime\"]\n",
    "    NUMERIC_COLS = (\n",
    "        set(CSV_COLUMNS) - set([LABEL_COLUMN, \"key\"]) - set(STRING_COLS)\n",
    "    )\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype=\"float32\")\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: layers.Input(name=colname, shape=(), dtype=\"string\")\n",
    "        for colname in STRING_COLS\n",
    "    })\n",
    "    \n",
    "    # Transforms\n",
    "    transformed, feature_columns = transform(\n",
    "        inputs, NUMERIC_COLS, STRING_COLS, nbuckets=nbuckets\n",
    "    )\n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "    \n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = layers.Dense(nodes, activation=\"relu\", name=\"h{}\".format(layer))(x)\n",
    "    output = tf.keras.layers.Dense(1, name=\"fare\")(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, output)\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    nbuckets = hparams[\"nbuckets\"]\n",
    "    lr = hparams[\"lr\"]\n",
    "    nnsize = hparams[\"nnsize\"]\n",
    "    eval_data_path = hparams[\"eval_data_path\"]\n",
    "    num_evals = hparams[\"num_evals\"]\n",
    "    num_examples_to_train_on = hparams[\"num_examples_to_train_on\"]\n",
    "    output_dir = hparams[\"output_dir\"]\n",
    "    train_data_path = hparams[\"train_data_path\"]\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    savedmodel_dir = os.path.join(output_dir, \"export/savedmodel\")\n",
    "    model_export_path = os.path.join(savedmodel_dir, timestamp)\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "    tensorboard_path = os.path.join(output_dir, \"tensorboard\")\n",
    "    \n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "        \n",
    "    model = build_dnn_model(nbuckets, nnsize, lr)\n",
    "    logging.info(model.summary())\n",
    "    \n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "    \n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "    \n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(tensorboard_path)\n",
    "    \n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2, #0=silent, #1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb]\n",
    "    )\n",
    "    \n",
    "    # Exporting the model with default serving function\n",
    "    tf.saved_model.save(model, model_export_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-circular",
   "metadata": {},
   "source": [
    "## Modify code to read data from and write checkpoint files to GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-theta",
   "metadata": {},
   "source": [
    "If you look closely above, you'll notice a new function, `train_and_evaluate` that wraps the code that actually trains the model. This allows us to parametrise the training by passing a dictionary of parameters to this function (e.g., `batch_size`, `num_examples_to_train_on`, `train_data_path`, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-absence",
   "metadata": {},
   "source": [
    "This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parameterising allows us to use the same code for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-dining",
   "metadata": {},
   "source": [
    "We specify these parameters at runtime via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with these parameters. This is the job of the `task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "portable-karma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "\n",
    "# The argparse module makes it easy to write user-friendly command-line interfaces. It parses the defined arguments\n",
    "# from the `sys.argv`\n",
    "# The argparse module also automatically generates help & usage messages and issues errors when users give the \n",
    "# program invalid arguments\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Write a `task.py` file for adding code to parse command line parameters and invoke `train_and_evaluate()` with\n",
    "# these parameters\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Batch size for training steps\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location pattern of eval files\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[32, 8]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help=\"Number of buckets to divide lat and lon with\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        help = \"learning rate for optimizer\",\n",
    "        type=float,\n",
    "        default=0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help=\"Number of times to evaluate model on eval data\",\n",
    "        type=int,\n",
    "        default=5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help=\"Number of examples to train on\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location pattern of train files containing eval URLs\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    hparams.pop(\"job-dir\", None)\n",
    "    \n",
    "    model.train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-corpus",
   "metadata": {},
   "source": [
    "## Run trainer module package locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-metropolitan",
   "metadata": {},
   "source": [
    "Now we can test our training code locally as follows using the local test data. We'll run a very small training job over a single file with a small batch size and one eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "assigned-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_datetime (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "euclidean (Lambda)              (None,)              0           pickup_longitude[0][0]           \n",
      "                                                                 pickup_latitude[0][0]            \n",
      "                                                                 dropoff_longitude[0][0]          \n",
      "                                                                 dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hourofday (Lambda)              (None,)              0           pickup_datetime[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 130)          1000000     scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 euclidean[0][0]                  \n",
      "                                                                 hourofday[0][0]                  \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "h0 (Dense)                      (None, 32)           4192        dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "h1 (Dense)                      (None, 8)            264         h0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fare (Dense)                    (None, 1)            9           h1[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 1,004,465\n",
      "Trainable params: 1,004,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "20/20 - 2s - loss: 271.0249 - rmse: 14.4909 - mse: 271.0249 - val_loss: 235.0190 - val_rmse: 13.4920 - val_mse: 235.0190\n",
      "\n",
      "Epoch 00001: saving model to models/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-09 22:57:45.278832: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-09 22:57:45.278868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-04-09 22:57:46.900605: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-04-09 22:57:46.900785: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-04-09 22:57:46.900801: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-04-09 22:57:46.900824: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MacBookPro): /proc/driver/nvidia/version does not exist\n",
      "2021-04-09 22:57:46.901010: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-04-09 22:57:46.901205: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-04-09 22:57:47.167900: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-04-09 22:57:47.167936: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-04-09 22:57:47.167994: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-04-09 22:57:47.195231: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-04-09 22:57:47.213618: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599700000 Hz\n",
      "2021-04-09 22:57:48.255354: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-04-09 22:57:48.255396: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-04-09 22:57:48.269483: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2021-04-09 22:57:48.271094: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-04-09 22:57:48.273941: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: models/tensorboard/train/plugins/profile/2021_04_09_22_57_48\n",
      "2021-04-09 22:57:48.275122: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.trace.json.gz\n",
      "2021-04-09 22:57:48.278014: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: models/tensorboard/train/plugins/profile/2021_04_09_22_57_48\n",
      "2021-04-09 22:57:48.278212: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.memory_profile.json.gz\n",
      "2021-04-09 22:57:48.278495: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: models/tensorboard/train/plugins/profile/2021_04_09_22_57_48Dumped tool data for xplane.pb to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.xplane.pb\n",
      "Dumped tool data for overview_page.pb to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to models/tensorboard/train/plugins/profile/2021_04_09_22_57_48/MacBookPro.kernel_stats.pb\n",
      "\n",
      "2021-04-09 22:57:50.174422: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Testing our training code locally\n",
    "EVAL_DATA_PATH=data/taxi-valid*\n",
    "TRAIN_DATA_PATH=data/taxi-train*\n",
    "OUTPUT_DIR=models\n",
    "\n",
    "test ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "\n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize 32 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-southeast",
   "metadata": {},
   "source": [
    "## Run your training package on Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-gambling",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode locally, you can run it on Cloud AI Platform. To submit to the CLoud we use `gcloud ai-platform jobs submit training [jobname]` and simply specify some parameters for AI Platform Training:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-standard",
   "metadata": {},
   "source": [
    "- jobid: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- region: Cloud region to train in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-alloy",
   "metadata": {},
   "source": [
    "The arguments before `-- \\` are AI Platform Training. The arguments after `-- \\` are sent to our `task.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-assignment",
   "metadata": {},
   "source": [
    "Because this is on the entire data set, it will take a while. You can monitor the job from the GCP console in the Cloud AI Platform section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u +%y%m%d_%H%M%S)\n",
    "JOBID=taxifare_$(date -y +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOBID \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=taxifare/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --region=${REGION} \\\n",
    "    -- \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data_path $TRAIN_DATA_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "    --num_evals $NUM_EVALS \\\n",
    "    --nbuckets $NBUCKETS \\\n",
    "    --lr $LR \\\n",
    "    --nnsize $NNSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-hometown",
   "metadata": {},
   "source": [
    "## Run your training package using a Docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-developer",
   "metadata": {},
   "source": [
    "AI Platform Training also supports training in custom containers, allowing users to bring their own Docker containers with any pre-installed ML framework or algorithm to run on AI Platform Training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-applicant",
   "metadata": {},
   "source": [
    "In this last section, we'll see how to submit a Cloud training job using a customised Docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-custody",
   "metadata": {},
   "source": [
    "Containerising your `taxifare/trainer` package involves 3 steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-palestinian",
   "metadata": {},
   "source": [
    "- Writing a `Dockerfile` in `/taxifare`\n",
    "- Buidling the Docker image\n",
    "- Pushing it to the Google Cloud container registry in your GCP project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-wyoming",
   "metadata": {},
   "source": [
    "The `Dockerfile` specifies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-divorce",
   "metadata": {},
   "source": [
    "1. How the container needs to be provisioned so that all the dependencies in our code are satisfied\n",
    "2. Where to copy our trainer Package in the container and how to install it (`pip install /trainer`)\n",
    "3. What command to run when the container is ran (the `ENTRYPOINT` line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prerequisite-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxifare/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/Dockerfile\n",
    "# Writing the Dockerfile\n",
    "FROM gcr.io/deeplearting-platform-release/tf2-cpu\n",
    "\n",
    "COPY . /code\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gclud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Pushing the docker image to Google Cloud container registry in your GCP project\n",
    "PROJECT_DIR=$(cd taxifare && pwd)\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "DOCKERFILE=$PROJECT_DIR/Dockerfile\n",
    "IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_NAME\n",
    "\n",
    "docker build $PROJECT_DIR -f $DOCKERFILE -t $IMAGE_URI\n",
    "\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-offset",
   "metadata": {},
   "source": [
    "## Train using a custom container on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-dream",
   "metadata": {},
   "source": [
    "To submit to the Cloud we use `gcloud ai-platform jobs submit training [jobname]` and simply specify some additional parameters for AI Platform Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-adventure",
   "metadata": {},
   "source": [
    "- jobname: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- master-image-uri: The URI of the Docker image we pushed in the Google Cloud registry\n",
    "- region: Cloud region to train in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-visibility",
   "metadata": {},
   "source": [
    "The arguments before `-- \\` are for AI Platform Training. The arguments after `-- \\` are sent to our `task.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-singer",
   "metadata": {},
   "source": [
    "You can track your job and view logs using [cloud console](https://console.cloud.google.com/mlengine/jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$PROJECT_ID\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model\n",
    "JOBID=taxifare_container_$(date -u + %y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# AI Platform machines to use for training\n",
    "MACHINE_TYPE=n1-standard-4\n",
    "SCALE_TIER=CUSTOM\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "IMAGE_NAME=taxifare_training_container\n",
    "IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_NAME\n",
    "\n",
    "gcloud beta ai-platform jobs submit training $JOBID \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --master-machine-type=$MACHINE_TYPE \\\n",
    "    --scale-tier=$SCALE_TIER \\\n",
    "    -- \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data-path $TRAIN_DATA_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_examples_to_trian_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "    --num_evals $NUM_EVALS \\\n",
    "    --nbuckets $NBUCKET \\\n",
    "    --nnsize $NNSIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

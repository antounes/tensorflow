{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pediatric-pierce",
   "metadata": {},
   "source": [
    "# **Load CSV and NumPy File Types in TensorFlow 2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-financing",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "1. Load a CSV file into a `tf.data.Dataset`\n",
    "2. Load NumPy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-garbage",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "We load CSV data from a file into a `tf.data.Dataset`. We also load NumPy data to a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-transparency",
   "metadata": {},
   "source": [
    "## **Load necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "humanitarian-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-track",
   "metadata": {},
   "source": [
    "Data can be loaded from an URL using `tf.keras.utils.get_file()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "durable-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "# Download a file from URL if it is not already in cache using `tf.keras.utils.get_file()`\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"test.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "political-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make NumPy values easier to read\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-deployment",
   "metadata": {},
   "source": [
    "## **Load data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-alfred",
   "metadata": {},
   "source": [
    "This section provides an example of how to load CSV data from a file into a `tf.data.Dataset`. The data used in this tutorial are taken from the Titanic passenger list. The model will predict the likelihood a passenger survived based on characteristics like age, geneder, ticket class, and whether the person was travelling alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-staff",
   "metadata": {},
   "source": [
    "To start, let's look at the top of the CSV file to see how it is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subject-begin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"
     ]
    }
   ],
   "source": [
    "# `head()` function is used to get the first n rows\n",
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-holmes",
   "metadata": {},
   "source": [
    "We can load this using Pandas, and pass the NumPy arrays to TensorFlow. If we need to scale up to a large set of files, or need a loader that integrates with TensorFlow and `tf.data`, then we can use the `tf.data.experimental.make_csv_dataset()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-philadelphia",
   "metadata": {},
   "source": [
    "The only column we need to identify explicitly is the one with the value that the model is intended to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opening-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"survived\"\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-aerospace",
   "metadata": {},
   "source": [
    "Now let's read the CSV data from the file and create a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "employed-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dataset() retrieves a Dataverse data set or its metadata\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    # Use `tf.data.experimental.make_csv_dataset()` to read CSV files into a data set\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to display\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recent-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key, value.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-indicator",
   "metadata": {},
   "source": [
    "Each item in the data set is a **batch**, represented as a tuple of `(examples, labels)`. The data from the examples is organised in column-based tensors (rather than row-based tensors), each with as many elements as the `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stuffed-married",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'male' b'female' b'male']\n",
      "age                 : [18. 27. 31. 16. 28.]\n",
      "n_siblings_spouses  : [0 0 1 0 0]\n",
      "parch               : [0 2 1 0 1]\n",
      "fare                : [11.5   11.133 37.004  7.75  33.   ]\n",
      "class               : [b'Second' b'Third' b'Second' b'Third' b'Second']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg' b'Queenstown' b'Southampton']\n",
      "alone               : [b'y' b'n' b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-rehabilitation",
   "metadata": {},
   "source": [
    "As we can see, the columns in the CSV are named. The data set constructor will pick these names up automatically. If the file we are working with does not contain the columns names in the first line, we shall pass them as a list of `str` to the `column_names` argument in the `tf.data.experimental.make_csv_dataset()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "congressional-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [20. 28. 19. 26. 22.]\n",
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [ 8.05  15.5    7.65   7.896  9.35 ]\n",
      "class               : [b'Third' b'Third' b'Third' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'F' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Queenstown' b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = [\"survived\", \"sex\", \"age\", \"n_siblings_spouses\", \"parch\", \n",
    "               \"fare\", \"class\", \"deck\", \"embark_town\", \"alone\"]\n",
    "\n",
    "# Pass column names as a list of `str` to the column_names argument\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-participation",
   "metadata": {},
   "source": [
    "This example is going to use all the available column. If we would like to omit some columns from the data set, we shall create a list of just the columns we plan to use, and pass it into the (optional) `select_columns` argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aerial-component",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [35. 25. 32. 62. 51.]\n",
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "class               : [b'Third' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "# If we need to omit some columns from the data set, we shall create a list of \n",
    "# just the columns we plan to use, and pass it into the (optional) `select_columns` argument of the constructor\n",
    "SELECT_COLUMNS = [\"survived\", \"age\", \"n_siblings_spouses\", \"class\", \"deck\", \"alone\"]\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-pottery",
   "metadata": {},
   "source": [
    "## **Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-survey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

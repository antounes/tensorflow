{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frozen-namibia",
   "metadata": {},
   "source": [
    "# **TensorFlow Dataset API**\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "1. Learn how to use `tf.data` to read data from memory\n",
    "2. Learn how to use `tf.data` in a training loop\n",
    "3. Learn how to use `tf.data` to read data from disk\n",
    "4. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression implemented previously so that it takes the form `tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original data set will be synthetic and read by the `tf.data` API directly from memory.\n",
    "\n",
    "In a second part, we will learn how to load a data set with the `tf.data` API when the data set resides on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sonic-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-active",
   "metadata": {},
   "source": [
    "## **Loading data from memory**\n",
    "\n",
    "### **Creating the data set**\n",
    "\n",
    "Let's create our synthetic data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "corrected-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] \n",
      "Y: [10. 12. 14. 16. 18. 20. 22. 24. 26. 28.]\n"
     ]
    }
   ],
   "source": [
    "N_POINTS = 10\n",
    "\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10\n",
    "\n",
    "print(\"X:\", X.numpy(), \"\\nY:\", Y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-validation",
   "metadata": {},
   "source": [
    "We begin with implementing a function that takes as input\n",
    "- our $X$ and $Y$ vectors of synthetic data generated from the linear function $y = 2x + 10$\n",
    "- the number of passes over the data set we want to train on (`epochs`)\n",
    "- the size of the batches of the data set (`batch_size`)\n",
    "\n",
    "and returns a `tf.data.Dataset`.\n",
    "\n",
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the data set was exhausted. If you want batches with the exact same number of elements per batch, we will have to discard the last batch by setting:\n",
    "\n",
    "`dataset = dataset.batch(batch_size, drop_remainder=True)`\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define create_dataset() procedure\n",
    "\n",
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    # Using the tf.data.Dataset.from_tensor_slices() method we are able to get slices of a `list` or `array`\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-memorial",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our data set in batches of 3 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afraid-cabinet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0. 1. 2.] y: [10. 12. 14.]\n",
      "x: [3. 4. 5.] y: [16. 18. 20.]\n",
      "x: [6. 7. 8.] y: [22. 24. 26.]\n",
      "x: [9. 0. 1.] y: [28. 10. 12.]\n",
      "x: [2. 3. 4.] y: [14. 16. 18.]\n",
      "x: [5. 6. 7.] y: [20. 22. 24.]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    # You can convert a native `tf.Tensor` to a NumPy `array` unsing `.numpy()` method\n",
    "    # Let's output the value of x and y\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE\n",
    "assert EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-commons",
   "metadata": {},
   "source": [
    "### **Loss function and gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "danish-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define loss_mse() procedure which will return computed mean of elements across dimensions of a tensor\n",
    "def loss_mse(X, Y, w0, w1):\n",
    "    return tf.reduce_mean(((w0 * X + w1) - Y)**2)\n",
    "\n",
    "# Let's define compute_gradients() procedure which will return value of recorded operations \n",
    "# for automatic differentiation\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-employee",
   "metadata": {},
   "source": [
    "### **Training loop**\n",
    "\n",
    "The main difference is that now, in the training loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sorted-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 MSE: 109.76800537109375 w0: 0.23999999463558197 w1: 0.4399999976158142\n",
      "STEP: 100 MSE: 9.363959312438965 w0: 2.55655837059021 w1: 6.674341678619385\n",
      "STEP: 200 MSE: 1.393267273902893 w0: 2.2146825790405273 w1: 8.717182159423828\n",
      "STEP: 300 MSE: 0.20730558037757874 w0: 2.082810878753662 w1: 9.505172729492188\n",
      "STEP: 400 MSE: 0.03084510937333107 w0: 2.03194260597229 w1: 9.809128761291504\n",
      "STEP: 500 MSE: 0.004589457996189594 w0: 2.012321710586548 w1: 9.926374435424805\n",
      "STEP: 600 MSE: 0.0006827632314525545 w0: 2.0047526359558105 w1: 9.971602439880371\n",
      "STEP: 700 MSE: 0.00010164897685172036 w0: 2.0018346309661865 w1: 9.989042282104492\n",
      "STEP: 800 MSE: 1.5142451957217418e-05 w0: 2.000706911087036 w1: 9.995771408081055\n",
      "STEP: 900 MSE: 2.256260358990403e-06 w0: 2.0002737045288086 w1: 9.998367309570312\n",
      "STEP: 1000 MSE: 3.3405058275093324e-07 w0: 2.000105381011963 w1: 9.999371528625488\n",
      "STEP: 1100 MSE: 4.977664502803236e-08 w0: 2.000040054321289 w1: 9.999757766723633\n",
      "STEP: 1200 MSE: 6.475602276623249e-09 w0: 2.0000154972076416 w1: 9.99991226196289\n"
     ]
    }
   ],
   "source": [
    "# Here we'll configure the data set so that it iterates 250 times over our synthetic data set in batches of 2\n",
    "\n",
    "# Set training environment\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "# Initialise weights\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "# Create data set in training environment\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Training loop\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    # Compute gradients with respect to model parameters w0 and w1\n",
    "    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1)\n",
    "    # Update w0 and w1\n",
    "    w0.assign_sub(dw0*LEARNING_RATE)\n",
    "    w1.assign_sub(dw1*LEARNING_RATE)\n",
    "    # Print metrics\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, w1)\n",
    "        print(\"STEP: {} MSE: {} w0: {} w1: {}\".format(step, loss, w0.numpy(), w1.numpy()))\n",
    "\n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-passing",
   "metadata": {},
   "source": [
    "## **Loading data from disk**\n",
    "\n",
    "### **Locating the csv files**\n",
    "\n",
    "We will start with the **taxifare dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "medieval-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 antounes antounes 25073872 mars  12 09:56 data/taxi-fare-test.csv\n",
      "-rw-rw-r-- 1 antounes antounes 25143609 mars  12 09:54 data/taxi-fare-train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-programmer",
   "metadata": {},
   "source": [
    "### **Use `tf.data` to read the CSV files**\n",
    "\n",
    "The `tf.data` API can easily read csv files using the helper function `tf.data.experimental.make_csv_dataset`. If you have `TFRecords` (which is recommended), you may use `tf.data.experimental.make_batched_features_dataset`\n",
    "\n",
    "The first step is to define:\n",
    "- the feature names into a list `CSV_COLUMNS`\n",
    "- their default values into a list `DEFAULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "northern-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"id\",\n",
    "    \"pickup_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"trip_duration\"\n",
    "]\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[\"na\"], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-vaccine",
   "metadata": {},
   "source": [
    "Let's now wrap the call to `make_csv_dataset` into its own function that will take only the file pattern (i.e. glob) where the data set files are to be located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "brutal-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: OrderedDict([(id, (1,)), (pickup_datetime, (1,)), (passenger_count, (1,)), (pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (trip_duration, (1,))]), types: OrderedDict([(id, tf.string), (pickup_datetime, tf.string), (passenger_count, tf.float32), (pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (trip_duration, tf.string)])>\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(pattern):\n",
    "    # The `tf.data.experimental.make_csv_dataset() method reads CSV files into a dataset`\n",
    "    return tf.data.experimental.make_csv_dataset(pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "tempds = create_dataset(\"data/taxi-train.csv\")\n",
    "# Let's output the value of `tempds`\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-killing",
   "metadata": {},
   "source": [
    "Note that this is a prefetched data set, where each element is an `OrderedDict` whose keys are the feature names and whose values are tensors of shape `(1,)` (i.e. vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "after-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.745857], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.97897], dtype=float32),\n",
      " 'id': array([b'id1709332'], dtype=object),\n",
      " 'passenger_count': array([5.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2016-03-21 09:15:46'], dtype=object),\n",
      " 'pickup_latitude': array([40.721836], dtype=float32),\n",
      " 'pickup_longitude': array([-74.00833], dtype=float32),\n",
      " 'trip_duration': array([b'1355'], dtype=object)}\n",
      "\n",
      "\n",
      "{'dropoff_latitude': array([40.76781], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.90583], dtype=float32),\n",
      " 'id': array([b'id0046394'], dtype=object),\n",
      " 'passenger_count': array([1.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2016-03-26 17:25:30'], dtype=object),\n",
      " 'pickup_latitude': array([40.75548], dtype=float32),\n",
      " 'pickup_longitude': array([-73.98386], dtype=float32),\n",
      " 'trip_duration': array([b'1545'], dtype=object)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's iterate over the first two elements of this dataset using `dataset.take(2)`\n",
    "# Then convert them to ordinary Python dictionary with numpy array as values for more readability\n",
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-fluid",
   "metadata": {},
   "source": [
    "### **Transforming the features**\n",
    "\n",
    "What we really need is a dictionary of features + a label. So, we have to do two things to the above dictionary:\n",
    "1. Remove the unwanted column `id`\n",
    "2. Keep the label separate from features\n",
    "\n",
    "Let's first implement a function that takes as input a row (represented as an `OrderedDict` in our `tf.data.Dataset` as above) and then returns a tuple with two elements:\n",
    "- The first element is the same `OrderedDict` with the label dropped\n",
    "- The second element is the label itself (`trip_duration`)\n",
    "\n",
    "Note that we will need to also remove the `id` and `pickup_datetime` columns, which we won't use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "prerequisite-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_COLS = [\"pickup_datetime\", \"id\"]\n",
    "\n",
    "# Let's define the features_and_labels() method\n",
    "def features_and_labels(row_data):\n",
    "    # The `.pop()` method will return item and drop from frame\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "    \n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-certification",
   "metadata": {},
   "source": [
    "Let's iterate over 2 examples from our `tempds` dataset and apply our `features_and_labels` function to each of the examples to make sure it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "complimentary-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>),\n",
      "             ('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.99823], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.745716], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.995], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.74646], dtype=float32)>)])\n",
      "tf.Tensor([b'358'], shape=(1,), dtype=string) \n",
      "\n",
      "OrderedDict([('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>),\n",
      "             ('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98789], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.71906], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.99243], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.74346], dtype=float32)>)])\n",
      "tf.Tensor([b'1466'], shape=(1,), dtype=string) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    assert UNWANTED_COLS[0] not in features.keys()\n",
    "    assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-protection",
   "metadata": {},
   "source": [
    "### **Batching**\n",
    "\n",
    "Let's now refactor our `create_dataset` function so that it takes an additional argument `batch_size` and batch the data correspondingly. We will also use the `features_and_labels` function we implemented for our `tf.data.Dataset` to produce tuples of features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "distinct-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the create_dataset() method\n",
    "def create_dataset(pattern, batch_size):\n",
    "    # The tf.data.experimental.make_csv_dataset() method reads CSV files into a `tf.data.Dataset`\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    return dataset.map(features_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-pressing",
   "metadata": {},
   "source": [
    "Let's check that our batches are of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "impossible-perry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.76498, 40.74407], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.97662, -73.9879 ], dtype=float32),\n",
      " 'passenger_count': array([1., 4.], dtype=float32),\n",
      " 'pickup_latitude': array([40.75322, 40.75848], dtype=float32),\n",
      " 'pickup_longitude': array([-73.97971, -73.98475], dtype=float32)}\n",
      "[b'341' b'463'] \n",
      "\n",
      "{'dropoff_latitude': array([40.739273, 40.7655  ], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.991905, -73.97568 ], dtype=float32),\n",
      " 'passenger_count': array([1., 5.], dtype=float32),\n",
      " 'pickup_latitude': array([40.72218, 40.75678], dtype=float32),\n",
      " 'pickup_longitude': array([-74.0106 , -73.99027], dtype=float32)}\n",
      "[b'1146' b'818'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "tempds = create_dataset(\"data/taxi-train*\", batch_size=2)\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(Y_batch.numpy(), \"\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-offset",
   "metadata": {},
   "source": [
    "### **Shuffling**\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so **averaging gradients across workers will help**. Also, during training, we will need to read the data indefinitely.\n",
    "\n",
    "Let's refactor our `create_dataset` function so that it shuffles the data, when the data set is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case when it needs to shuffle the data (`mode == \"train\"`) from when it shouldn't (`mode == \"eval\"`).\n",
    "\n",
    "Also, before returning we will want to prefetch 1 data point ahead of time (`dataset.prefetch(1)`) to speed-up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "painful-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    \n",
    "    # The `map()` function executes a specified function on each item of an iterable\n",
    "    # The item is sent to the function as a parameter\n",
    "    dataset = dataset.map(features_and_labels).cache()\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-attribute",
   "metadata": {},
   "source": [
    "Let's check that our function works well in both modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "southwest-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(OrderedDict([('passenger_count', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>), ('pickup_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.98663, -73.98484], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.74597 , 40.711388], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-74.002754, -73.98135 ], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.739914, 40.722054], dtype=float32)>)]), <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'720', b'288'], dtype=object)>)]\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"data/taxi-train*\", batch_size=2, mode=\"train\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "arctic-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(OrderedDict([('passenger_count', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>), ('pickup_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.980125, -73.95641 ], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.765305, 40.81344 ], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.800186, -73.953   ], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.708366, 40.80246 ], dtype=float32)>)]), <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'1468', b'297'], dtype=object)>)]\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset(\"data/taxi-train*\", batch_size=2, mode=\"eval\")\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-event",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "official-citizenship",
   "metadata": {},
   "source": [
    "# **Load CSV and NumPy File Types in TensorFlow 2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-interference",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "1. Load a CSV file into a `tf.data.Dataset`\n",
    "2. Load NumPy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-daughter",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "We load CSV data from a file into a `tf.data.Dataset`. We also load NumPy data to a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-basin",
   "metadata": {},
   "source": [
    "## **Load necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "personal-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-coalition",
   "metadata": {},
   "source": [
    "Data can be loaded from an URL using `tf.keras.utils.get_file()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "freelance-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "# Download a file from URL if it is not already in cache using `tf.keras.utils.get_file()`\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"test.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "primary-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make NumPy values easier to read\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-robinson",
   "metadata": {},
   "source": [
    "## **Load data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-george",
   "metadata": {},
   "source": [
    "This section provides an example of how to load CSV data from a file into a `tf.data.Dataset`. The data used in this tutorial are taken from the Titanic passenger list. The model will predict the likelihood a passenger survived based on characteristics like age, geneder, ticket class, and whether the person was travelling alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-temperature",
   "metadata": {},
   "source": [
    "To start, let's look at the top of the CSV file to see how it is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equal-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"
     ]
    }
   ],
   "source": [
    "# `head()` function is used to get the first n rows\n",
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-courage",
   "metadata": {},
   "source": [
    "We can load this using Pandas, and pass the NumPy arrays to TensorFlow. If we need to scale up to a large set of files, or need a loader that integrates with TensorFlow and `tf.data`, then we can use the `tf.data.experimental.make_csv_dataset()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-seventh",
   "metadata": {},
   "source": [
    "The only column we need to identify explicitly is the one with the value that the model is intended to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "looking-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"survived\"\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-glance",
   "metadata": {},
   "source": [
    "Now let's read the CSV data from the file and create a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "special-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dataset() retrieves a Dataverse data set or its metadata\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    # Use `tf.data.experimental.make_csv_dataset()` to read CSV files into a data set\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to display\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subject-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key, value.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-marathon",
   "metadata": {},
   "source": [
    "Each item in the data set is a **batch**, represented as a tuple of `(examples, labels)`. The data from the examples is organised in column-based tensors (rather than row-based tensors), each with as many elements as the `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "experienced-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'male' b'female' b'male']\n",
      "age                 : [18. 27. 31. 16. 28.]\n",
      "n_siblings_spouses  : [0 0 1 0 0]\n",
      "parch               : [0 2 1 0 1]\n",
      "fare                : [11.5   11.133 37.004  7.75  33.   ]\n",
      "class               : [b'Second' b'Third' b'Second' b'Third' b'Second']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg' b'Queenstown' b'Southampton']\n",
      "alone               : [b'y' b'n' b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-sitting",
   "metadata": {},
   "source": [
    "As we can see, the columns in the CSV are named. The data set constructor will pick these names up automatically. If the file we are working with does not contain the columns names in the first line, we shall pass them as a list of `str` to the `column_names` argument in the `tf.data.experimental.make_csv_dataset()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "classified-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [20. 28. 19. 26. 22.]\n",
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [ 8.05  15.5    7.65   7.896  9.35 ]\n",
      "class               : [b'Third' b'Third' b'Third' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'F' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Queenstown' b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = [\"survived\", \"sex\", \"age\", \"n_siblings_spouses\", \"parch\", \n",
    "               \"fare\", \"class\", \"deck\", \"embark_town\", \"alone\"]\n",
    "\n",
    "# Pass column names as a list of `str` to the column_names argument\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-terrain",
   "metadata": {},
   "source": [
    "This example is going to use all the available column. If we would like to omit some columns from the data set, we shall create a list of just the columns we plan to use, and pass it into the (optional) `select_columns` argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "accessory-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [35. 25. 32. 62. 51.]\n",
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "class               : [b'Third' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "# If we need to omit some columns from the data set, we shall create a list of \n",
    "# just the columns we plan to use, and pass it into the (optional) `select_columns` argument of the constructor\n",
    "SELECT_COLUMNS = [\"survived\", \"age\", \"n_siblings_spouses\", \"class\", \"deck\", \"alone\"]\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-description",
   "metadata": {},
   "source": [
    "## **Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-prefix",
   "metadata": {},
   "source": [
    "A CSV file can contain a variety of data types. Typically we want to convert from those mixed types to a fixed length vector before feeding the data into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-paradise",
   "metadata": {},
   "source": [
    "TensorFlow has a built-in system for describing common input conversions: `tf.feature_column`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-brazilian",
   "metadata": {},
   "source": [
    "We can preprocess data using any tool we like (like [nltk](https://www.nltk.org/) or [sklearn](https://scikit-learn.org/stable/)), and just pass the processed output to TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-house",
   "metadata": {},
   "source": [
    "The primary advantage of doing preprocessing inside the model is that when we export the model it includes the preprocessing. This way we can pass the raw data directly to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-therapy",
   "metadata": {},
   "source": [
    "### **Continuous data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-silver",
   "metadata": {},
   "source": [
    "If the data is already in an appropriate numeric format, we can pack the data into a vector before passing it off to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "structural-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [28. 25. 28. 24. 28.]\n",
      "n_siblings_spouses  : [0. 0. 0. 0. 0.]\n",
      "parch               : [0. 0. 0. 0. 0.]\n",
      "fare                : [7.896 7.65  7.75  8.05  7.725]\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = [\"survived\", \"age\", \"n_siblings_spouses\", \"parch\", \"fare\"]\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(\n",
    "    train_file_path,\n",
    "    select_columns=SELECT_COLUMNS,\n",
    "    column_defaults=DEFAULTS)\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "quick-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, label_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-orleans",
   "metadata": {},
   "source": [
    "Here's a simple function that will pack together all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "higher-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `pack()` function will pack together all the columns\n",
    "def pack(features, label):\n",
    "    # `tf.stack()` stacks a list of rank-R tensors into one rank-(R+1) tensor\n",
    "    return tf.stack(list(features.values()), axis=-1), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-pilot",
   "metadata": {},
   "source": [
    "Apply this to each element of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "qualified-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 34.      1.      1.     32.5  ]\n",
      " [ 21.      0.      0.      7.75 ]\n",
      " [ 15.      0.      1.    211.337]\n",
      " [ 28.      0.      0.      7.733]\n",
      " [ 28.      0.      0.     12.35 ]]\n",
      "\n",
      "[1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "packed_dataset = temp_dataset.map(pack)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-humidity",
   "metadata": {},
   "source": [
    "If we have mixed data types we may want to separate out these simple-numeric fields. The `tf.feature_column` API can handle them, but this incurs some overhead and should be avoided unless really necessary. Let's switch back to the mixed data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "expected-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'female' b'male' b'male']\n",
      "age                 : [29. 47. 41. 28. 48.]\n",
      "n_siblings_spouses  : [1 0 0 0 0]\n",
      "parch               : [0 0 5 0 0]\n",
      "fare                : [21.    15.    39.688  7.75   7.854]\n",
      "class               : [b'Second' b'Second' b'Third' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Queenstown' b'Southampton']\n",
      "alone               : [b'n' b'y' b'n' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "express-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-suicide",
   "metadata": {},
   "source": [
    "So define a more general preprocessor that selects a list of numeric features and pack them into a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "announced-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=1)\n",
    "        features[\"numeric\"] = numeric_features\n",
    "        \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "changed-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = [\"age\", \"n_siblings_spouses\", \"parch\", \"fare\"]\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))\n",
    "\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dedicated-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'male' b'male' b'male']\n",
      "class               : [b'Third' b'Second' b'Third' b'Third' b'First']\n",
      "deck                : [b'unknown' b'E' b'unknown' b'unknown' b'D']\n",
      "embark_town         : [b'Cherbourg' b'Queenstown' b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'n']\n",
      "numeric             : [[29.     0.     0.     7.896]\n",
      " [28.     0.     0.    12.35 ]\n",
      " [28.     0.     0.     7.896]\n",
      " [19.     0.     0.     7.896]\n",
      " [23.     0.     1.    63.358]]\n"
     ]
    }
   ],
   "source": [
    "show_batch(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "grave-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(packed_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-plaza",
   "metadata": {},
   "source": [
    "**Data Normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-majority",
   "metadata": {},
   "source": [
    "Continuous data should **always be normalised**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "electoral-liability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.631308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>34.385399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.511818</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>0.792999</td>\n",
       "      <td>54.597730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  n_siblings_spouses       parch        fare\n",
       "count  627.000000          627.000000  627.000000  627.000000\n",
       "mean    29.631308            0.545455    0.379585   34.385399\n",
       "std     12.511818            1.151090    0.792999   54.597730\n",
       "min      0.750000            0.000000    0.000000    0.000000\n",
       "25%     23.000000            0.000000    0.000000    7.895800\n",
       "50%     28.000000            0.000000    0.000000   15.045800\n",
       "75%     35.000000            1.000000    0.000000   31.387500\n",
       "max     80.000000            8.000000    5.000000  512.329200"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "# Pandas module `read_csv()` function reads the CSV file into a DataFrame object\n",
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "national-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = np.array(desc.T[\"mean\"])\n",
    "STD = np.array(desc.T[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "prerequisite-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_data(data, mean, std):\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acoustic-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.631  0.545  0.38  34.385] [12.512  1.151  0.793 54.598]\n"
     ]
    }
   ],
   "source": [
    "print(MEAN, STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-liberty",
   "metadata": {},
   "source": [
    "Now let's create a numeric column. The `tf.feature_columns.numeric_column()` API accepts a `normalizer_fn` argument, which will be run on each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-fortune",
   "metadata": {},
   "source": [
    "Bind the `MEAN` and `STD` variables to the `normalizer_fn` using `functools.partial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "billion-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericColumn(key='numeric', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x7fec312f7a60>, mean=array([29.631,  0.545,  0.38 , 34.385]), std=array([12.512,  1.151,  0.793, 54.598])))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we just created\n",
    "# Bind the `MEAN` and `STD` variables to the `normalizer_fn` using `functools.partial`\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "# `tf.feature_columns.numeric_column()` represents real-valued or numerical features\n",
    "numeric_column = tf.feature_column.numeric_column(\n",
    "    \"numeric\", normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-beijing",
   "metadata": {},
   "source": [
    "When we train the model, we shall include this feature column to select and center this block of numeric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "environmental-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[33.   ,  0.   ,  0.   ,  5.   ],\n",
       "       [19.   ,  0.   ,  0.   ,  6.75 ],\n",
       "       [35.   ,  0.   ,  0.   , 26.288],\n",
       "       [17.   ,  4.   ,  2.   ,  7.925],\n",
       "       [22.   ,  0.   ,  0.   , 10.517]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch[\"numeric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "emotional-myanmar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.269, -0.474, -0.479, -0.538],\n",
       "       [-0.85 , -0.474, -0.479, -0.506],\n",
       "       [ 0.429, -0.474, -0.479, -0.148],\n",
       "       [-1.01 ,  3.001,  2.043, -0.485],\n",
       "       [-0.61 , -0.474, -0.479, -0.437]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `tf.keras.layers.DenseFeatures()` produces a dense Tensor based on given `feature_columns`\n",
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-penny",
   "metadata": {},
   "source": [
    "The mean-based normalisation used here requires knowing the means of each column ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-dietary",
   "metadata": {},
   "source": [
    "### **Categorical data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-contractor",
   "metadata": {},
   "source": [
    "Some of the columns in the CSV data are categorical columns. That is, the content should be one of a limited set of options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-basketball",
   "metadata": {},
   "source": [
    "Let's use the `tf.feature_column` API to create a collection with a `tf.feature_column.indicator_column` for each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "compliant-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    \"sex\": [\"male\", \"female\"],\n",
    "    \"class\": [\"First\", \"Second\", \"Third\"],\n",
    "    \"deck\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n",
    "    \"embark_town\": [\"Cherbourg\", \"Southampton\", \"Queenstown\"],\n",
    "    \"alone\": [\"y\", \"n\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "annoying-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "    # Use the `tf.feature_column` API to create a collection with a `tf.feature_column.indicator_column`\n",
    "    # for each categorical column\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab\n",
    "    )\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "paperback-celtic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we've just created\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "burning-potter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# `tf.keras.layers.DenseFeatures()` produces a dense Tensor based on given feature_columns.\n",
    "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "print(categorical_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-burns",
   "metadata": {},
   "source": [
    "This will be part of a data processing input layer when we build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-bahrain",
   "metadata": {},
   "source": [
    "### **Combined preprocessing layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-milton",
   "metadata": {},
   "source": [
    "Let's add the two `feature_column` collections and pass them to a `tf.keras.layers.DenseFeatures()` to create an input layer that will extract and preprocess both input types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "integrated-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the two `feature_column` collections\n",
    "# Pass them to a `tf.keras.layers.DenseFeatures()` to create an input layer\n",
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "completed-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.     0.     1.     0.     0.     0.     1.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     1.     0.     0.269 -0.474\n",
      " -0.479 -0.538  1.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessing_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-procedure",
   "metadata": {},
   "source": [
    "### **Next Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-candidate",
   "metadata": {},
   "source": [
    "A next step would be to build a `tf.keras.Sequential` neural network model, starting with a `preprocessing_layer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-reward",
   "metadata": {},
   "source": [
    "## **Load NumPy data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-young",
   "metadata": {},
   "source": [
    "### **Load necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hairy-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-copyright",
   "metadata": {},
   "source": [
    "### **Load data from `.npz` file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-accessory",
   "metadata": {},
   "source": [
    "We use the `MNIST` data set in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "\n",
    "# `tf.keras.utils.get_file()` downloads a file from an URL if it is not already in cache\n",
    "path = tf.keras.utils.get_file(\"mnist.npz\", DATA_URL)\n",
    "with np.load(path) as data:\n",
    "    train_examples = data[\"x_train\"]\n",
    "    train_labels = data[\"y_train\"]\n",
    "    test_examples = data[\"x_test\"]\n",
    "    test_labels = data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-lindsay",
   "metadata": {},
   "source": [
    "## **Load NumPy arrays with `tf.data.Dataset`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-choir",
   "metadata": {},
   "source": [
    "Assuming you have an array of examples and a corresponding array of labels, pass the two arrays as a tuple into `tf.data.Dataset.from_tensor_slices()` to create a `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expected-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of `tf.data.Dataset.from_tensor_slices()` method, we can get the slices of an array in the form of objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

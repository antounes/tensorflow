{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coupled-gilbert",
   "metadata": {},
   "source": [
    "# **Introducing the Keras Sequential API**\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "1. Build a DNN model using the Keras Sequential API\n",
    "2. Learn how to use feature columns in a Keras model\n",
    "3. Learn how to train a model with Keras\n",
    "4. Learn how to save/load and deploy a Keras model on GCP\n",
    "5. Learn how to deploy and make predictions with a Keras model\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "The Keras Sequential API allows you to create TensorFlow models **layer-by-layer**. This is useful for building most kind of meachine learning models but **it does not allow you to create model that share layers, re-use layers or have multiple inputs or outputs**.\n",
    "\n",
    "In this lab, we'll see how to build a simple deep neural network (DNN) model using Keras sequential API and feature columns. Once we have trained our model, we will deploy it using AI Platform and see how to call our model for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-economics",
   "metadata": {},
   "source": [
    "## **Load raw data**\n",
    "\n",
    "We will use the taxi data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amino-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 122288\n",
      "-rw-rw-r-- 1 antounes antounes    33558 mars  14 21:22 images.tfrecords\n",
      "-rw-r--r-- 1 antounes antounes 44706590 mars  16 15:54 taxi-test.csv\n",
      "-rw-r--r-- 1 antounes antounes 79468840 mars  16 15:53 taxi-train.csv\n",
      "-rw-rw-r-- 1 antounes antounes  1003818 mars  14 21:01 test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "judicial-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/taxi-test.csv <==\n",
      "passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude\n",
      "1,-73.9881286621094,40.7320289611816,-73.9901733398438,40.7566795349121\n",
      "1,-73.9642028808594,40.6799926757813,-73.9598083496094,40.655403137207\n",
      "1,-73.9974365234375,40.7375831604004,-73.9861602783203,40.7295227050781\n",
      "1,-73.9560699462891,40.771900177002,-73.9864273071289,40.73046875\n",
      "1,-73.97021484375,40.761474609375,-73.9615097045899,40.7558898925781\n",
      "1,-73.9913024902344,40.7497978210449,-73.9805145263672,40.786548614502\n",
      "1,-73.9783096313477,40.7415504455566,-73.9520721435547,40.7170028686523\n",
      "2,-74.0127105712891,40.7015266418457,-73.9864807128906,40.7195091247559\n",
      "2,-73.9923324584961,40.7305107116699,-73.875617980957,40.8752136230469\n",
      "\n",
      "==> data/taxi-train.csv <==\n",
      "passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,trip_duration\n",
      "1,-73.9821548461914,40.767936706543,-73.9646301269531,40.7656021118164,455\n",
      "1,-73.9804153442383,40.7385635375977,-73.9994812011719,40.7311515808105,663\n",
      "1,-73.9790267944336,40.7639389038086,-74.0053329467773,40.7100868225098,2124\n",
      "1,-74.0100402832031,40.719970703125,-74.0122680664063,40.7067184448242,429\n",
      "1,-73.9730529785156,40.7932090759277,-73.9729232788086,40.7825202941895,435\n",
      "6,-73.9828567504883,40.7421951293945,-73.9920806884766,40.7491836547852,443\n",
      "4,-73.9690170288086,40.7578392028809,-73.957405090332,40.7658958435059,341\n",
      "1,-73.9692764282227,40.797779083252,-73.9224700927734,40.7605590820313,1551\n",
      "1,-73.9994812011719,40.7383995056152,-73.9857864379883,40.7328147888184,255\n"
     ]
    }
   ],
   "source": [
    "!head data/taxi*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-revelation",
   "metadata": {},
   "source": [
    "## **Use `tf.data` to read the CSV files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "scientific-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"trip_duration\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "# The .pop() method will return item and drop from frame. \n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "\n",
    "    return features, label\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    # The `tf.data.experimental.make_csv_dataset()` method reads CSV files into a data set\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS\n",
    "    )\n",
    "    \n",
    "    # The `.map()` function executes a specified function for each item in the iterable\n",
    "    # The item is sent to the function as a parameter\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "    # The `.shuffle()` method takes a sequence (list, string or tuple) and reorganise the order of the items\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "        \n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-lawsuit",
   "metadata": {},
   "source": [
    "## **Build a simple Keras DNN model**\n",
    "\n",
    "We will use feature columns to connect our raw data to our Keras DNN model. **Feature columns make it easy to perform common types of feature engineering on your raw data**. For example, you can one-hot-encode categorical data, create feature crosses, embeddings, and more.\n",
    "\n",
    "In our case we won't do any feature engineering. However, we still need to create a list of feature columns to specify the numeric values which will be passed on to our model. To do this, we use `tf.feature_column.numeric_column()`.\n",
    "\n",
    "We use a Python dictionary comprehension to create the feature columns for our model, which is just an elegant alternative to a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "representative-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `INPUT_COLS`\n",
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\"\n",
    "]\n",
    "\n",
    "# Create input layer of feature columns\n",
    "feature_columns = {\n",
    "    colname: tf.feature_column.numeric_column(colname) for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-pharmacy",
   "metadata": {},
   "source": [
    "Let's create the DNN model. The Sequential model is a **linear stack of layers** and when building a model using the Sequential API, you configure each layer of the model in turn. Once all the layers have been added, you compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "armed-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Keras DNN model using Sequential API\n",
    "model = Sequential([\n",
    "    DenseFeatures(feature_columns=feature_columns.values()),\n",
    "    Dense(units=32, activation=\"relu\", name=\"h1\"),\n",
    "    Dense(units=8, activation=\"relu\", name=\"h2\"),\n",
    "    Dense(units=1, activation=\"linear\", name=\"output\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-request",
   "metadata": {},
   "source": [
    "To prepare the model for training, you must configure the learning process. This is done using the `.compile()` method. The `.compile()` method takes 3 arguments:\n",
    "- An **optimiser**. This could be the string identifier of an existing optimiser (such as `rmsprop` or `adagrad`), or an instance of the `Optimiser` class\n",
    "- A **loss function**. This is the objective that the model is trying to minimise during training. It can be the string identifier of an existing loss function from the `Loss` class (such as `categorical_crossentropy` or `mse`), or it can be a custom objective function\n",
    "- A **list of metrics**. For any machine learning problem, you will want a set of metrics to evaluate your model. It can be the string identifier of an existing metric or a custom metric function\n",
    "\n",
    "We will add a custom metric called `rmse` to our list of metrics which will return the root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quick-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom evaluation metric\n",
    "\n",
    "def rmse(y_true, y_hat):\n",
    "    return tf.sqrt(tf.reduce_mean(y_hat - y_true))\n",
    "\n",
    "# Compile the Keras model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-printer",
   "metadata": {},
   "source": [
    "## **Train the model**\n",
    "\n",
    "To train the model, Keras provides three functions that can be used:\n",
    "1. `.fit()` for training a model for a fixed number of epochs (iterations on a data set)\n",
    "2. `.fit_generator()` for training a model on data yielded batch-by-batch by a generator\n",
    "3. `.train_on_batch()` runs a single gradient update on a single batch of data\n",
    "\n",
    "The `.fit()` function works well for small data sets which can fit entirely in memory. However, for large data sets (or if you need to manipulate the training data on the fly via data augmentation, etc.), you will need to use `.fit_generator()` instead. The `.train_on_batch()` method is for more fine-grained control over training and accepts only a single batch of data.\n",
    "\n",
    "Our taxi data set is small enough to fit in memory, so we could use `.fit()` to train the model. Our `create_dataset()` function above generates batches of training examples, so we could also use `.fit_generator()`. In fact, when calling `.fit()` the method inspects the data, and if it's a generator (as our data set is), it will invoke automatically `.fit_generator()` for training.\n",
    "\n",
    "We start by setting up some parameters for our training job and create the data generators for the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "detailed-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5 # Training data set will repeat, wrap around\n",
    "NUM_EVALS = 50 # How many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000 # Enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern=\"data/taxi-train.csv\",\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern=\"data/taxi-test.csv\",\n",
    "    batch_size=1000,\n",
    "    mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-provincial",
   "metadata": {},
   "source": [
    "There are various arguments you can set when calling the `.fit()` method. Here `x` specifies the input data which in our case is a `tf.data.Dataset` returning a tuple `(inputs, targets)`. The `steps_per_epoch` parameter is used to mark the end of training for a single epoch. Here we are training for `NUM_EVALS` epochs. Lastly, for the `callback` argument we specify a Tensorboard callback so we can inspect Tensorboard after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "swedish-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.48 µs\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
      "Consider rewriting this model with the Functional API.\n",
      "1/1 [==============================] - ETA: 0s - loss: 20643.4434 - rmse: 6.2075 - mse: 20643.4434WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Expect 6 fields but have 5 in record\n\t [[node IteratorGetNext (defined at <ipython-input-15-49f8458fecb4>:6) ]] [Op:__inference_test_function_1297]\n\nFunction call stack:\ntest_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49f8458fecb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLOGDIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./taxi_trained\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the sequential model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = model.fit(x=trainds,\n\u001b[0m\u001b[1;32m      7\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EVALS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1131\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m               *args, **kwds)\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m       return self._concrete_stateful_fn._call_flat(\n\u001b[0m\u001b[1;32m    895\u001b[0m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Expect 6 fields but have 5 in record\n\t [[node IteratorGetNext (defined at <ipython-input-15-49f8458fecb4>:6) ]] [Op:__inference_test_function_1297]\n\nFunction call stack:\ntest_function\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "LOGDIR = \"./taxi_trained\"\n",
    "# Train the sequential model\n",
    "history = model.fit(x=trainds,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=NUM_EVALS,\n",
    "                    validation_data=evalds,\n",
    "                    callbacks=[TensorBoard(LOGDIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-carolina",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

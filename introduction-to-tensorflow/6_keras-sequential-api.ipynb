{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-infrared",
   "metadata": {},
   "source": [
    "# **Introducing the Keras Sequential API**\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "1. Build a DNN model using the Keras Sequential API\n",
    "2. Learn how to use feature columns in a Keras model\n",
    "3. Learn how to train a model with Keras\n",
    "4. Learn how to save/load and deploy a Keras model on GCP\n",
    "5. Learn how to deploy and make predictions with a Keras model\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "The Keras Sequential API allows you to create TensorFlow models **layer-by-layer**. This is useful for building most kind of meachine learning models but **it does not allow you to create model that share layers, re-use layers or have multiple inputs or outputs**.\n",
    "\n",
    "In this lab, we'll see how to build a simple deep neural network (DNN) model using Keras sequential API and feature columns. Once we have trained our model, we will deploy it using AI Platform and see how to call our model for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepting-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-february",
   "metadata": {},
   "source": [
    "## **Load raw data**\n",
    "\n",
    "We will use the taxi data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recorded-establishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 171324\n",
      "-rw-rw-r-- 1 antounes antounes     33558 mars  14 21:22 images.tfrecords\n",
      "-rw-r--r-- 1 antounes antounes  63460629 mars  12 13:46 taxi-test.csv\n",
      "-rw-r--r-- 1 antounes antounes 110926109 mars  12 13:44 taxi-train.csv\n",
      "-rw-rw-r-- 1 antounes antounes   1003818 mars  14 21:01 test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesser-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/taxi-test.csv <==\n",
      "id,pickup_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude\n",
      "id3004672,2016-06-30 23:59:58,1,-73.9881286621094,40.7320289611816,-73.9901733398438,40.7566795349121\n",
      "id3505355,2016-06-30 23:59:53,1,-73.9642028808594,40.6799926757813,-73.9598083496094,40.655403137207\n",
      "id1217141,2016-06-30 23:59:47,1,-73.9974365234375,40.7375831604004,-73.9861602783203,40.7295227050781\n",
      "id2150126,2016-06-30 23:59:41,1,-73.9560699462891,40.771900177002,-73.9864273071289,40.73046875\n",
      "id1598245,2016-06-30 23:59:33,1,-73.97021484375,40.761474609375,-73.9615097045899,40.7558898925781\n",
      "id0668992,2016-06-30 23:59:30,1,-73.9913024902344,40.7497978210449,-73.9805145263672,40.786548614502\n",
      "id1765014,2016-06-30 23:59:15,1,-73.9783096313477,40.7415504455566,-73.9520721435547,40.7170028686523\n",
      "id0898117,2016-06-30 23:59:09,2,-74.0127105712891,40.7015266418457,-73.9864807128906,40.7195091247559\n",
      "id3905224,2016-06-30 23:58:55,2,-73.9923324584961,40.7305107116699,-73.875617980957,40.8752136230469\n",
      "\n",
      "==> data/taxi-train.csv <==\n",
      "id,pickup_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,trip_duration\n",
      "id2875421,2016-03-14 17:24:55,1,-73.9821548461914,40.767936706543,-73.9646301269531,40.7656021118164,455\n",
      "id2377394,2016-06-12 00:43:35,1,-73.9804153442383,40.7385635375977,-73.9994812011719,40.7311515808105,663\n",
      "id3858529,2016-01-19 11:35:24,1,-73.9790267944336,40.7639389038086,-74.0053329467773,40.7100868225098,2124\n",
      "id3504673,2016-04-06 19:32:31,1,-74.0100402832031,40.719970703125,-74.0122680664063,40.7067184448242,429\n",
      "id2181028,2016-03-26 13:30:55,1,-73.9730529785156,40.7932090759277,-73.9729232788086,40.7825202941895,435\n",
      "id0801584,2016-01-30 22:01:40,6,-73.9828567504883,40.7421951293945,-73.9920806884766,40.7491836547852,443\n",
      "id1813257,2016-06-17 22:34:59,4,-73.9690170288086,40.7578392028809,-73.957405090332,40.7658958435059,341\n",
      "id1324603,2016-05-21 07:54:58,1,-73.9692764282227,40.797779083252,-73.9224700927734,40.7605590820313,1551\n",
      "id1301050,2016-05-27 23:12:23,1,-73.9994812011719,40.7383995056152,-73.9857864379883,40.7328147888184,255\n"
     ]
    }
   ],
   "source": [
    "!head data/taxi*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-search",
   "metadata": {},
   "source": [
    "## **Use `tf.data` to read the CSV files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "round-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"trip_duration\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"id\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
    "UNWANTED_COLS = [\"pickup_datetime\", \"id\"]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    # The `.pop()` method will return item and drop from frame\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "        \n",
    "    return features, label\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    # The `tf.data.experimental.make_csv_dataset()` method reads CSV files into a data set\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS\n",
    "    )\n",
    "    \n",
    "    # The `.map()` function executes a specified function for each item in the iterable\n",
    "    # The item is sent to the function as a parameter\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "    # The `.shuffle()` method takes a sequence (list, string or tuple) and reorganise the order of the items\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "        \n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-association",
   "metadata": {},
   "source": [
    "## **Build a simple Keras DNN model**\n",
    "\n",
    "We will use feature columns to connect our raw data to our Keras DNN model. **Feature columns make it easy to perform common types of feature engineering on your raw data**. For example, you can one-hot-encode categorical data, create feature crosses, embeddings, and more.\n",
    "\n",
    "In our case we won't do any feature engineering. However, we still need to create a list of feature columns to specify the numeric values which will be passed on to our model. To do this, we use `tf.feature_column.numeric_column()`.\n",
    "\n",
    "We use a Python dictionary comprehension to create the feature columns for our model, which is just an elegant alternative to a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demonstrated-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature names into a list `INPUT_COLS`\n",
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\"\n",
    "]\n",
    "\n",
    "# Create input layer of feature columns\n",
    "feature_columns = {\n",
    "    colname: tf.feature_column.numeric_column(colname) for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-success",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
